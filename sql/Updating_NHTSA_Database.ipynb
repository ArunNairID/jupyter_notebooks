{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Updating NHTSA Complaints sqlite Database</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will show you how to create a sqlite table consisting of customer complaints filed with NHTSA (National Highway Traffic Safety Administration).  The metadata on NHTSA's csv file (information about the data), can be viewed [here](http://www-odi.nhtsa.dot.gov/downloads/folders/Complaints/CMPL.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example assumes that a ```nhtsa.db``` sqlite database was already created.  With sqlite installed, at the terminal, just enter:<br>\n",
    "***promtp>***```sqlite3 nhtsa.db```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why [sqlite](www.sqlite.org)?**\n",
    "- data is relatively small\n",
    "- database will be used by a small number of users\n",
    "- Python already has built-in library to interact with a sqlite database\n",
    "- it is free and used in production in several companies and web sites (it is probably the most used database of all time: used in most if not all smart phones, browsers, embedded devices, etc ([source](http://www.sqlite.org/mostdeployed.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few different ways I could have obtained the data and manage the rest of my work flow.  I could manually go to NHTSA's site, download the zip file somewhere onto my computer, then extract it, massage or clean the data, and then import the data into a database.  Since I would have to repeat this process on a monthly basis, I wanted to automate this process with a single script so that I can perhaps take advantage of Windows Scheduler or Linux CRON to schedule this process automatically if I wanted to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the zip file in memory, then extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I have enough RAM on my computer, I will be downloading the zip file and holding it in memory instead of physically writing/creating a file on disk.  However, the contents of the zip file will be saved on the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download and extraction completed\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib import request\n",
    "import datetime as dt\n",
    "\n",
    "start = dt.datetime.now()\n",
    "\n",
    "url = request.urlopen('http://www-odi.nhtsa.dot.gov/downloads/folders/Complaints/FLAT_CMPL.zip')\n",
    "zipfile_in_memory = ZipFile(BytesIO(url.read()))\n",
    "zipfile_in_memory.extractall(r'D:\\temp')\n",
    "zipfile_in_memory.close()\n",
    "print(\"Download and extraction completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not sure why NHTSA omitted the column names from the csv file since they've already defined them in the data description file, so I had to create them myself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I've created ```columns``` list to contain column names that will be used for the ```complaints``` table in my sqlite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": true
    }
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'CMPLID',\n",
    "    'ODINO',\n",
    "    'MFR_NAME',\n",
    "    'MAKETXT',\n",
    "    'MODELTXT',\n",
    "    'YEARTXT',\n",
    "    'CRASH',\n",
    "    'FAILDATE',\n",
    "    'FIRE',\n",
    "    'INJURED',\n",
    "    'DEATHS',\n",
    "    'COMPDESC',\n",
    "    'CITY',\n",
    "    'STATE',\n",
    "    'VIN',\n",
    "    'DATEA',\n",
    "    'LDATE',\n",
    "    'MILES',\n",
    "    'OCCURENCES',\n",
    "    'CDESCR',\n",
    "    'CMPL_TYPE',\n",
    "    'POLICE_RPT_YN',\n",
    "    'PURCH_DT',\n",
    "    'ORIG_OWNER_YN',\n",
    "    'ANTI_BRAKES_YN',\n",
    "    'CRUISE_CONT_YN',\n",
    "    'NUM_CYLS',\n",
    "    'DRIVE_TRAIN',\n",
    "    'FUEL_SYS',\n",
    "    'FUEL_TYPE',\n",
    "    'TRANS_TYPE',\n",
    "    'VEH_SPEED',\n",
    "    'DOT',\n",
    "    'TIRE_SIZE',\n",
    "    'LOC_OF_TIRE',\n",
    "    'TIRE_FAIL_TYPE',\n",
    "    'ORIG_EQUIP_YN',\n",
    "    'MANUF_DT',\n",
    "    'SEAT_TYPE',\n",
    "    'RESTRAINT_TYPE',\n",
    "    'DEALER_NAME',\n",
    "    'DEALER_TEL',\n",
    "    'DEALER_CITY',\n",
    "    'DEALER_STATE',\n",
    "    'DEALER_ZIP',\n",
    "    'PROD_TYPE',\n",
    "    'REPAIRED_YN',\n",
    "    'MEDICAL_ATTN',\n",
    "    'VEHICLES_TOWED_YN'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the sqlite3 database and read in the csv file, in chunks at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create/populate the ```complaints``` table in the nhtsa.db sqlite database.  I used Pandas read_csv ```chunksize``` parameter due to the size of the csv file.  With chunking, I will add 20K rows at a time and append them to the complaints table instead of attempting to add all rows into the table.  Since we are processing 20K rows at a time, this technique will work with out-of-core or larger-than-memory csv files (data that is larger than RAM, but fits on hard drive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 seconds: completed 1340000 rows"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "conn = sqlite3.connect(r'D:\\NHTSA\\nhtsa.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Since we are going to load/re-create the complaint's table in its entirety, DROP it\n",
    "cursor.execute('DROP TABLE IF EXISTS complaints')\n",
    "\n",
    "chunksize = 20000\n",
    "j = 0\n",
    "\n",
    "begin = dt.datetime.now()\n",
    "\n",
    "# use the columns list to define the column names of the complaints table\n",
    "for df in pd.read_csv(r'D:\\temp\\FLAT_CMPL.txt', names=columns, dtype=object, chunksize=chunksize, \n",
    "                      delimiter='\\t', iterator=True, encoding='ISO-8859-1', error_bad_lines=False):    \n",
    "    j+=1\n",
    "    # To print on same line, use '\\r' and end='' option with the print function\n",
    "    print('\\r'+'{} seconds: completed {} rows'.format((dt.datetime.now() - begin).seconds, j*chunksize),end='')\n",
    "\n",
    "    df.to_sql('complaints', conn, if_exists='append', index=False)\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To improve query performance, create indices based on most frequently used columns used for filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on my past querying history, I've been filtering a lot based on combinations of year, make, model year, and failure date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(r'D:\\NHTSA\\nhtsa.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('CREATE INDEX make ON complaints (MAKETXT)')\n",
    "cursor.execute('CREATE INDEX addeddate ON complaints (DATEA)')\n",
    "cursor.execute('CREATE INDEX faildate ON complaints (FAILDATE)')\n",
    "cursor.execute('CREATE INDEX compdesc ON complaints (COMPDESC)')\n",
    "cursor.execute('CREATE INDEX \"make-faildate\" ON complaints (MAKETXT, FAILDATE)')\n",
    "cursor.execute('CREATE INDEX \"year-make-model\" ON complaints (MAKETXT, MODELTXT, YEARTXT)')\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time (hr:min:sec): 0:13:08.021469\n"
     ]
    }
   ],
   "source": [
    "print(\"Total elapsed time (hr:min:sec):\", dt.datetime.now() - start)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/18b3d717c198162ecab0c019f6015bf1"
  },
  "gist": {
   "data": {
    "description": "sql/Creating_SqliteDB_From_NHTSA_Data.ipynb",
    "public": true
   },
   "id": "18b3d717c198162ecab0c019f6015bf1"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
